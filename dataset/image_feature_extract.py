# %%
from regex import B, F
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor
import cv2
import numpy as np
import matplotlib.pyplot as plt
from segment_anything.modeling import mask_decoder
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import sklearn.decomposition
import os
import random
from sklearn.cluster import KMeans

# %%
# generate human image without background
def generate_human_img(img_dir, mask_dir, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    # sample four images
    image_files = os.listdir(img_dir)
    # sampled_images = random.sample(image_files, 4)
    sampled_images = ['capture-f00011.png', 'capture-f00051.png', 'capture-f00101.png', 'capture-f00151.png']
    sampled_masks = [img.replace('capture', 'mask') for img in sampled_images]  
    sampled_image_paths = [os.path.join(img_dir, img) for img in sampled_images]
    sampled_mask_paths = [os.path.join(mask_dir, img) for img in sampled_masks]
    for sampled_image_path, sampled_mask_path in zip(sampled_image_paths, sampled_mask_paths):
        image = cv2.imread(sampled_image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask_human = cv2.imread(sampled_mask_path)
        mask_human = cv2.cvtColor(mask_human, cv2.COLOR_BGR2GRAY)
        mask_human = mask_human[..., None]
        mask_human = np.where(mask_human > 0, True, False)
        dark_bg = np.zeros_like(image)
        image_human = image * mask_human + dark_bg * (1 - mask_human)
        Image.fromarray(image_human.astype(np.uint8)).save(os.path.join(save_dir, sampled_image_path.split('/')[-1]))

generate_human_img('/home/zychen/Documents/feature-3dgs/data/4ddress/00122/Outer/Take9/Capture/0076/images', 
                   '/home/zychen/Documents/feature-3dgs/data/4ddress/00122/Outer/Take9/Capture/0076/masks', 
                   'data/4ddress/human_process')


# %%
# visualize human image
fig = plt.figure(figsize=(8, 12))
for i, img in enumerate(os.listdir('data/4ddress/human_process')):
    image = cv2.imread(os.path.join('data/4ddress/human_process', img))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    ax = fig.add_subplot(max(1, 4//2), 2, i+1)
    ax.imshow(image)
    ax.axis('off')

fig.savefig(f'data/4ddress/human_process_vis.png')
plt.show()

# 设置保存图片的目录  
save_dir = 'data/4ddress/human_process_vis'  
os.makedirs(save_dir, exist_ok=True)  # 如果目录不存在，则创建它  
  
# 遍历指定目录下的所有图像文件  
for i, img in enumerate(os.listdir('data/4ddress/human_process')):  
    # 读取图像  
    image_path = os.path.join('data/4ddress/human_process', img)  
    image = cv2.imread(image_path)  
    # 将图像从BGR格式转换为RGB格式  
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  
      
    # 创建一个新的图形实例  
    fig, ax = plt.subplots(1, 1, figsize=(6, 8))  # 1行1列，大小可以根据需要调整  
      
    # 显示图像  
    ax.imshow(image_rgb)  
      
    # 去掉坐标轴  
    ax.axis('off')  
      
    # 设置保存图片的文件名（例如使用原始文件名）  
    save_path = os.path.join(save_dir, f'image_{i+1}.png')  
      
    # 保存图形到文件  
    fig.savefig(save_path, bbox_inches='tight', pad_inches=0) 
      
    # 关闭图形实例以释放资源（可选，因为plt.show()后会自动关闭）  
    # 但在这里我们不需要显示图形，所以直接关闭  
    plt.close(fig)  

# %%
def obtain_feature_mask(img_dir, mask_dir, mask_resize_shape, downsample_size=None):
    # generate mask on feature map space
    human_masks = []
    for img in os.listdir(img_dir):
        human_mask = cv2.imread(os.path.join(mask_dir, img.replace('capture', 'mask')))
        human_mask = cv2.cvtColor(human_mask, cv2.COLOR_BGR2GRAY)
        human_mask = human_mask[..., None]
        human_mask = np.where(human_mask > 0, True, False)
        human_masks.append(human_mask)
    human_mask = np.stack(human_masks, axis=0) # (batch_size, h, w, 1)
    human_mask = torch.from_numpy(human_mask).permute(0, 3, 1, 2).float() # (batch_size, 1, h, w)
    # 使用 interpolate 函数进行缩放
    human_mask = nn.functional.interpolate(human_mask, size=mask_resize_shape, mode='nearest')
    # 对 mask 进行平均池化操作
    if downsample_size is not None:
        human_mask = nn.functional.avg_pool2d(human_mask, kernel_size=downsample_size, stride=downsample_size)
    # 根据平均值是否大于 0.5 来判断块中的多数元素是 0 还是 1
    feature_mask = (human_mask > 0.5) # (batch_size, 1, patch_h, patch_w)

    return feature_mask

def feature_pca(human_feature, model_name, feature_mask=None):
    # PCA for feature inferred
    pca = sklearn.decomposition.PCA(n_components=3, random_state=42)

    batch_size, patch_h, patch_w, feat_dim = human_feature.shape
    features = human_feature.reshape(-1, feat_dim).cpu()
    # human feature mask given by the image mask label
    features_fg = feature_mask.reshape(-1,)

    if feature_mask is None:
        # human feature mask generated by the first main component in PCA
        pca.fit(features)
        pca_features = pca.transform(features)
        # visualize PCA components for finding a proper threshold
        plt.plot()
        plt.hist(pca_features[:, 0])
        plt.title('histogram of the first pca component')
        plt.show()
        # uncomment below to plot the first pca component
        # for i in range(batch_size):
        #     plt.subplot(max(1, batch_size//2), 2, i+1)
        #     plt.imshow(pca_features[i * patch_h * patch_w: (i+1) * patch_h * patch_w, 0].reshape(patch_h, patch_w))
        # plt.title('first pca component visualization')
        # plt.show()
        features_fg = pca_features[:, 0] < -10

    # plot the features_fg
    fig = plt.figure()
    for i in range(batch_size):
        ax = fig.add_subplot(max(1, batch_size//2), 2, i+1)        
        ax.imshow(features_fg[i * patch_h * patch_w: (i+1) * patch_h * patch_w].reshape(patch_h, patch_w))
    fig.suptitle('foreground feature mask')
    plt.show()

    # PCA for only foreground patches
    pca.fit(features[features_fg]) 
    pca_features_rem = pca.transform(features[features_fg])
    # scaling
    for i in range(3):
        q1, q99 = np.percentile(pca_features_rem[:, i], [1, 99])
        feature_pca_postprocess_sub = q1
        feature_pca_postprocess_div = (q99 - q1)
        pca_features_rem[:, i] = (pca_features_rem[:, i] - feature_pca_postprocess_sub) / feature_pca_postprocess_div
        pca_features_rem[:, i] = pca_features_rem[:, i].clip(0.0, 1.0)

    pca_features_rgb = np.zeros((features.shape[0], 3))
    pca_features_rgb[~features_fg] = 0
    pca_features_rgb[features_fg] = pca_features_rem

    pca_features_rgb = pca_features_rgb.reshape(batch_size, patch_h, patch_w, 3)
    fig = plt.figure(figsize=(8, 12))
    for i in range(batch_size):
        ax = fig.add_subplot(max(1, batch_size//2), 2, i+1)
        ax.imshow(pca_features_rgb[i][..., ::-1])
        ax.axis('off')
    fig.suptitle(f'foreground {model_name} feature pca visualization')
    fig.savefig(f'data/4ddress/human_feature_{model_name}_pca.png')
    plt.show()

    # 设置保存图片的目录  
    save_dir = 'data/4ddress/human_process_feature_vis'  
    os.makedirs(save_dir, exist_ok=True)  # 如果目录不存在，则创建它  
     
    for i in range(batch_size):  

        fig, ax = plt.subplots(1, 1, figsize=(6, 8))
        
        ax.imshow(pca_features_rgb[i][..., ::-1])
        
        # 去掉坐标轴  
        ax.axis('off')  
        
        # 设置保存图片的文件名（例如使用原始文件名）  
        save_path = os.path.join(save_dir, f'image_{i+1}.png')  
        
        # 保存图形到文件  
        fig.savefig(save_path, bbox_inches='tight', pad_inches=0)  # bbox_inches='tight' 和 pad_inches=0 用于减少图像周围的空白  
        
        # 关闭图形实例以释放资源（可选，因为plt.show()后会自动关闭）  
        # 但在这里我们不需要显示图形，所以直接关闭  
        plt.close(fig)  

def feature_kmeans(human_feature, feature_mask, model_name):
    # kmeans clustering on foreground patch features
    SURFACE_LABEL = ['skin', 'hair', 'shoe', 'upper', 'lower', 'outer']
    SURFACE_LABEL_COLOR = np.array([[128, 128, 128], [255, 128, 0], [128, 0, 255], [180, 50, 50], [50, 180, 50], [0, 128, 255]])
    batch_size, patch_h, patch_w, feat_dim = human_feature.shape
    features = human_feature.reshape(-1, feat_dim).cpu()
    labels = np.ones(features.shape[0]) * -1
    kmeans = KMeans(n_clusters=len(SURFACE_LABEL), random_state=42).fit(features[feature_mask.reshape(-1,)])  
    labels[feature_mask.reshape(-1,)] = kmeans.labels_  
    labels = labels.reshape(batch_size, patch_h, patch_w)

    # render pixel labels (bs, h, w) to colors (bs, h, w, 3)
    # init parser_images with black background
    images_kmeans = np.zeros((*labels.shape[:3], 3))
    # loop over all parser images
    for nv in range(images_kmeans.shape[0]):
        images_kmeans[nv][labels[nv] == -1] = [0, 0, 0]
        for nl in range(len(SURFACE_LABEL)):
            images_kmeans[nv][labels[nv] == nl] = SURFACE_LABEL_COLOR[nl]
    images_kmeans = images_kmeans.astype(np.uint8)

    fig = plt.figure(figsize=(8, 12))
    for i in range(batch_size):
        ax = fig.add_subplot(max(1, batch_size//2), 2, i+1)
        ax.imshow(images_kmeans[i][..., ::-1])
        ax.axis('off')
    fig.suptitle(f'foreground {model_name} feature kmeans visualization')
    fig.savefig(f'data/4ddress/human_feature_{model_name}_kmeans.png')
    plt.show()

# def feature_pca_v1(human_feature, model_name):
#     if human_feature.ndim == 3:
#         fmap = human_feature[None, :, :, :] # torch.Size([1, 512, h, w]) for lseg  [1, 256, h, w] for seg
#     else:
#         fmap = human_feature
#     fmap = nn.functional.normalize(fmap, dim=1)
#     pca = sklearn.decomposition.PCA(3, random_state=42)
#     f_samples = fmap.permute(0, 2, 3, 1).reshape(-1, fmap.shape[1])[::3].numpy()
#     transformed = pca.fit_transform(f_samples)
#     feature_pca_mean = torch.tensor(f_samples.mean(0)).float()
#     feature_pca_components = torch.tensor(pca.components_).float()
#     q1, q99 = np.percentile(transformed, [1, 99])
#     feature_pca_postprocess_sub = q1
#     feature_pca_postprocess_div = (q99 - q1)
#     del f_samples
#     vis_feature = (fmap.permute(0, 2, 3, 1).reshape(-1, fmap.shape[1]) - feature_pca_mean[None, :]) @ feature_pca_components.T
#     vis_feature = (vis_feature - feature_pca_postprocess_sub) / feature_pca_postprocess_div
#     vis_feature = vis_feature.clamp(0.0, 1.0).float().reshape((fmap.shape[2], fmap.shape[3], 3))
#     Image.fromarray((vis_feature.numpy() * 255).astype(np.uint8)).save(f"data/4ddress/human_feature_{model_name}_vis.png")


# %%
# sam =============================================================================
# sam segmentation for human
# def show_anns(anns):
#     if len(anns) == 0:
#         return
#     sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
#     ax = plt.gca()
#     ax.set_autoscale_on(False)

#     img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
#     img[:,:,3] = 0
#     for ann in sorted_anns:
#         m = ann['segmentation']
#         color_mask = np.concatenate([np.random.random(3), [0.35]])
#         img[m] = color_mask
#     ax.imshow(img)

checkpoint_dir = 'encoders/sam_encoder/checkpoints/sam_vit_h_4b8939.pth'
sam = sam_model_registry["vit_h"](checkpoint=checkpoint_dir)
sam.to(device='cuda:4')
sam.eval()

# human segmentation
# image_human = Image.open('data/4ddress/human_image.png')
# mask_generator = SamAutomaticMaskGenerator(sam)
# masks = mask_generator.generate(image_human.astype('uint8'))
# plt.figure(figsize=(10,10))
# plt.imshow(image_human)
# show_anns(masks)
# plt.axis('off')
# plt.savefig('data/4ddress/human_seg.png', bbox_inches='tight', pad_inches=0)


# sam feature map
checkpoint_dir = 'encoders/sam_encoder/checkpoints/sam_vit_h_4b8939.pth'
sam = sam_model_registry["vit_h"](checkpoint=checkpoint_dir)
sam.to(device='cuda:4')
sam.eval()

human_features = []
for img in os.listdir('data/4ddress/human_process'):
    image_human = cv2.imread(os.path.join('data/4ddress/human_process', img))
    image_human = cv2.cvtColor(image_human, cv2.COLOR_BGR2RGB)
    predictor = SamPredictor(sam)
    predictor.set_image(image_human)
    image_embedding_tensor = torch.tensor(predictor.get_image_embedding().cpu().numpy()[0])
    img_h, img_w, _ = image_human.shape
    _, fea_h, fea_w = image_embedding_tensor.shape
    # determine the cropped height or width
    if min(img_w, img_h) == img_w:
        cropped_side = int(fea_h / img_h * img_w + 0.5)
        human_feature = image_embedding_tensor[:, :, :cropped_side]
    else:
        cropped_side = int(fea_w / img_w * img_h + 0.5)
        human_feature = image_embedding_tensor[:, :cropped_side, :]
    human_features.append(human_feature)
human_feature = torch.stack(human_features, axis=0) # (batch_size, 256, 64, 47)

# normalize the feature map
human_feature = nn.functional.normalize(human_feature, dim=1)

# %%
# obtain human feature mask
scale = 1024 * 1.0 / max(img_h, img_w)  # max length of original image shape
newh, neww = img_h * scale, img_w * scale
neww = int(neww + 0.5)
newh = int(newh + 0.5)

feature_mask = obtain_feature_mask('data/4ddress/human_process', 'data/4ddress/00122/Outer/Take9/Capture/0076/masks',
                                   (newh, neww), 16)
# %%
feature_pca(human_feature, 'sam', feature_mask)

feature_kmeans(human_feature, feature_mask, 'sam')


# %%
# dino ============================================================================
# dino human feature map
dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')
dino_model.eval()
dino_model.to('cuda:4')

patch_h, patch_w = 64, 64
feat_dim = 1024

# image transformations
transform = transforms.Compose([
                                transforms.ToTensor(),              
                                transforms.Resize((patch_h * 14, patch_w * 14)), #should be multiple of model patch_size=14
                                transforms.CenterCrop((patch_h * 14, patch_w * 14)), #should be multiple of model patch_size=14                             
                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
                                ])
human_images = []
for img in os.listdir('data/4ddress/human_process'):
    human_image = Image.open(os.path.join('data/4ddress/human_process', img))
    human_image = np.array(human_image)
    human_image = transform(human_image)
    human_images.append(human_image)
human_image = torch.stack(human_images, axis=0)
human_image = human_image.to('cuda:4')
batch_size = human_image.shape[0]

# human_image = cv2.imread('/home/zychen/Documents/feature-3dgs/data/4ddress/human_process/capture-f00017.png') # 1280, 940
# human_image = cv2.cvtColor(human_image, cv2.COLOR_BGR2RGB)
# human_image = transform(human_image).unsqueeze(0).to('cuda:4')
# batch_size = human_image.shape[0]

with torch.no_grad():
    features_dict = dino_model.forward_features(human_image)
    human_feature = features_dict['x_norm_patchtokens']
    human_feature = human_feature.reshape(batch_size, patch_h, patch_w, -1).contiguous()

# %%
feature_mask = obtain_feature_mask('data/4ddress/human_process', 'data/4ddress/00122/Outer/Take9/Capture/0076/masks',
                                   (patch_h*14, patch_w*14), 14)

feature_pca(human_feature, 'dino', feature_mask)

feature_kmeans(human_feature, feature_mask, 'dino')


# %%
# lseg ============================================================================
# use encoders/lseg_encoder/encode_imgaes.py to generate lseg feature map first
human_features = []
for file in os.listdir('data/4ddress/human_process_lseg_feature'):
    human_feature = torch.load(os.path.join('data/4ddress/human_process_lseg_feature', file))
    human_features.append(human_feature)
human_feature = torch.stack(human_features, dim=0) # (4, 512, 360, 480)

human_feature = nn.functional.normalize(human_feature.to(torch.float32), dim=1)

# %%
feature_mask = obtain_feature_mask('data/4ddress/human_process', 'data/4ddress/00122/Outer/Take9/Capture/0076/masks',
                                   (360, 480))

feature_pca(human_feature, 'lseg', feature_mask)

feature_kmeans(human_feature, feature_mask, 'lseg')

